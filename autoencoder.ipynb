{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'    \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as p\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.layers as l\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14337, 55)"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('input/data_before_direct.csv', parse_dates=['start_time']).sort_values('start_time').fillna(0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2015-07-25 17:51:36.806330'),\n",
       " Timestamp('2015-07-26 14:40:52.280043'))"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.start_time.min(), data.start_time.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "corte = datetime.datetime.strptime('2015-07-26 13:35', '%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11073, 55)\n",
      "(3264, 55)\n"
     ]
    }
   ],
   "source": [
    "train_data = data[data.start_time < corte]\n",
    "test_data = data[data.start_time > corte]\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [c for c in data.columns if 'mode' in c]\n",
    "num_cols = [c for c in data.columns if c not in labels and c not in ['start_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11073, 48)\n",
      "(3264, 48)\n"
     ]
    }
   ],
   "source": [
    "scaler = p.StandardScaler()\n",
    "scaler.fit(data[num_cols])\n",
    "\n",
    "train_x = scaler.transform(train_data[num_cols])\n",
    "test_x = scaler.transform(test_data[num_cols])\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: Tensor(\"input_1:0\", shape=(?, 48), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-854-40e0ffa4d79c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hidden_in'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model.add(l.Dropout(0.1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# model.add(l.Dense(80, activation='relu'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aferraresso\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    428\u001b[0m             raise TypeError('The added layer must be '\n\u001b[0;32m    429\u001b[0m                             \u001b[1;34m'an instance of class Layer. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                             'Found: ' + str(layer))\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;31m# First layer in model: check that it is an input layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: Tensor(\"input_1:0\", shape=(?, 48), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(l.Input(input_shape=(train_x.shape[1],)))\n",
    "model.add(l.Dense(40, name='hidden_in', activation='relu'))\n",
    "# model.add(l.Dropout(0.1))\n",
    "# model.add(l.Dense(80, activation='relu'))\n",
    "# model.add(l.Dropout(0.1))\n",
    "model.add(l.Dense(30, name='bottleneck', activation='linear'))\n",
    "model.add(l.Dense(40, name='hidden_out', activation='relu'))\n",
    "# model.add(l.Dense(100, activation='relu'))\n",
    "model.add(l.Dense(train_x.shape[1], name='output', activation='linear'))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.edgecolor']='black'\n",
    "plt.rcParams['axes.facecolor']='white'\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "def plotit(history, min_epoch=0, show_all=True):\n",
    "    legends = []\n",
    "    if show_all:\n",
    "        for val in history.history:\n",
    "            plt.plot(history.history[val][min_epoch:])\n",
    "            legends.append(val)\n",
    "    else:\n",
    "        plt.plot(history.history['loss'][min_epoch:])\n",
    "        plt.plot(history.history['val_loss'][min_epoch:])\n",
    "        legends = ['loss']\n",
    "    plt.title('MSE del modelo')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11073 samples, validate on 3264 samples\n",
      "Epoch 1/600\n",
      "11073/11073 [==============================] - 2s 184us/step - loss: 1.1405 - val_loss: 0.5346\n",
      "Epoch 2/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 1.0608 - val_loss: 0.5305\n",
      "Epoch 3/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 1.0131 - val_loss: 0.5275\n",
      "Epoch 4/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.9787 - val_loss: 0.5250\n",
      "Epoch 5/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.9512 - val_loss: 0.5227\n",
      "Epoch 6/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.9272 - val_loss: 0.5205\n",
      "Epoch 7/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.9058 - val_loss: 0.5184\n",
      "Epoch 8/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.8861 - val_loss: 0.5163\n",
      "Epoch 9/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.8673 - val_loss: 0.5140\n",
      "Epoch 10/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.8487 - val_loss: 0.5115\n",
      "Epoch 11/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.8300 - val_loss: 0.5088\n",
      "Epoch 12/600\n",
      "11073/11073 [==============================] - 0s 13us/step - loss: 0.8110 - val_loss: 0.5059\n",
      "Epoch 13/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.7916 - val_loss: 0.5027\n",
      "Epoch 14/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.7720 - val_loss: 0.4996\n",
      "Epoch 15/600\n",
      "11073/11073 [==============================] - 0s 12us/step - loss: 0.7524 - val_loss: 0.4965\n",
      "Epoch 16/600\n",
      "11073/11073 [==============================] - 0s 12us/step - loss: 0.7330 - val_loss: 0.4932\n",
      "Epoch 17/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.7137 - val_loss: 0.4900\n",
      "Epoch 18/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.6947 - val_loss: 0.4868\n",
      "Epoch 19/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.6760 - val_loss: 0.4836\n",
      "Epoch 20/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.6582 - val_loss: 0.4804\n",
      "Epoch 21/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.6411 - val_loss: 0.4774\n",
      "Epoch 22/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.6250 - val_loss: 0.4744\n",
      "Epoch 23/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.6098 - val_loss: 0.4715\n",
      "Epoch 24/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.5955 - val_loss: 0.4687\n",
      "Epoch 25/600\n",
      "11073/11073 [==============================] - 0s 13us/step - loss: 0.5821 - val_loss: 0.4661\n",
      "Epoch 26/600\n",
      "11073/11073 [==============================] - 0s 12us/step - loss: 0.5696 - val_loss: 0.4637\n",
      "Epoch 27/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.5579 - val_loss: 0.4614\n",
      "Epoch 28/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.5471 - val_loss: 0.4594\n",
      "Epoch 29/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.5369 - val_loss: 0.4574\n",
      "Epoch 30/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.5273 - val_loss: 0.4556\n",
      "Epoch 31/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.5183 - val_loss: 0.4539\n",
      "Epoch 32/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.5098 - val_loss: 0.4522\n",
      "Epoch 33/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.5018 - val_loss: 0.4507\n",
      "Epoch 34/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.4942 - val_loss: 0.4492\n",
      "Epoch 35/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4870 - val_loss: 0.4478\n",
      "Epoch 36/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.4801 - val_loss: 0.4464\n",
      "Epoch 37/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4736 - val_loss: 0.4451\n",
      "Epoch 38/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.4673 - val_loss: 0.4438\n",
      "Epoch 39/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4613 - val_loss: 0.4425\n",
      "Epoch 40/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.4555 - val_loss: 0.4413\n",
      "Epoch 41/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4499 - val_loss: 0.4402\n",
      "Epoch 42/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4444 - val_loss: 0.4390\n",
      "Epoch 43/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4392 - val_loss: 0.4379\n",
      "Epoch 44/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4341 - val_loss: 0.4369\n",
      "Epoch 45/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4292 - val_loss: 0.4359\n",
      "Epoch 46/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4245 - val_loss: 0.4349\n",
      "Epoch 47/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4200 - val_loss: 0.4338\n",
      "Epoch 48/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4156 - val_loss: 0.4328\n",
      "Epoch 49/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4112 - val_loss: 0.4318\n",
      "Epoch 50/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4071 - val_loss: 0.4308\n",
      "Epoch 51/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.4031 - val_loss: 0.4298\n",
      "Epoch 52/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3991 - val_loss: 0.4287\n",
      "Epoch 53/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.3953 - val_loss: 0.4276\n",
      "Epoch 54/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3916 - val_loss: 0.4265\n",
      "Epoch 55/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3880 - val_loss: 0.4254\n",
      "Epoch 56/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3845 - val_loss: 0.4243\n",
      "Epoch 57/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.3810 - val_loss: 0.4232\n",
      "Epoch 58/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3777 - val_loss: 0.4222\n",
      "Epoch 59/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3745 - val_loss: 0.4211\n",
      "Epoch 60/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3713 - val_loss: 0.4201\n",
      "Epoch 61/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3682 - val_loss: 0.4191\n",
      "Epoch 62/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3652 - val_loss: 0.4181\n",
      "Epoch 63/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3622 - val_loss: 0.4172\n",
      "Epoch 64/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3593 - val_loss: 0.4162\n",
      "Epoch 65/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3565 - val_loss: 0.4153\n",
      "Epoch 66/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3537 - val_loss: 0.4144\n",
      "Epoch 67/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3509 - val_loss: 0.4135\n",
      "Epoch 68/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3482 - val_loss: 0.4126\n",
      "Epoch 69/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3456 - val_loss: 0.4117\n",
      "Epoch 70/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3430 - val_loss: 0.4108\n",
      "Epoch 71/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3404 - val_loss: 0.4099\n",
      "Epoch 72/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3378 - val_loss: 0.4090\n",
      "Epoch 73/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3353 - val_loss: 0.4082\n",
      "Epoch 74/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3329 - val_loss: 0.4074\n",
      "Epoch 75/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3304 - val_loss: 0.4065\n",
      "Epoch 76/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3280 - val_loss: 0.4057\n",
      "Epoch 77/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3256 - val_loss: 0.4048\n",
      "Epoch 78/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3233 - val_loss: 0.4040\n",
      "Epoch 79/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3210 - val_loss: 0.4032\n",
      "Epoch 80/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3186 - val_loss: 0.4024\n",
      "Epoch 81/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3164 - val_loss: 0.4016\n",
      "Epoch 82/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3141 - val_loss: 0.4007\n",
      "Epoch 83/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3118 - val_loss: 0.3999\n",
      "Epoch 84/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3096 - val_loss: 0.3991\n",
      "Epoch 85/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3074 - val_loss: 0.3983\n",
      "Epoch 86/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3052 - val_loss: 0.3975\n",
      "Epoch 87/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.3030 - val_loss: 0.3967\n",
      "Epoch 88/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.3009 - val_loss: 0.3959\n",
      "Epoch 89/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2988 - val_loss: 0.3951\n",
      "Epoch 90/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.2967 - val_loss: 0.3943\n",
      "Epoch 91/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.2946 - val_loss: 0.3935\n",
      "Epoch 92/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.2926 - val_loss: 0.3927\n",
      "Epoch 93/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2905 - val_loss: 0.3919\n",
      "Epoch 94/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2885 - val_loss: 0.3912\n",
      "Epoch 95/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2865 - val_loss: 0.3904\n",
      "Epoch 96/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2845 - val_loss: 0.3896\n",
      "Epoch 97/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2825 - val_loss: 0.3889\n",
      "Epoch 98/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2805 - val_loss: 0.3881\n",
      "Epoch 99/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2786 - val_loss: 0.3873\n",
      "Epoch 100/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2766 - val_loss: 0.3865\n",
      "Epoch 101/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2747 - val_loss: 0.3857\n",
      "Epoch 102/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2728 - val_loss: 0.3849\n",
      "Epoch 103/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2710 - val_loss: 0.3841\n",
      "Epoch 104/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2691 - val_loss: 0.3834\n",
      "Epoch 105/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2673 - val_loss: 0.3826\n",
      "Epoch 106/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2654 - val_loss: 0.3818\n",
      "Epoch 107/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2636 - val_loss: 0.3811\n",
      "Epoch 108/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2617 - val_loss: 0.3802\n",
      "Epoch 109/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2598 - val_loss: 0.3795\n",
      "Epoch 110/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2580 - val_loss: 0.3788\n",
      "Epoch 111/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2562 - val_loss: 0.3781\n",
      "Epoch 112/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2545 - val_loss: 0.3774\n",
      "Epoch 113/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2527 - val_loss: 0.3767\n",
      "Epoch 114/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2510 - val_loss: 0.3759\n",
      "Epoch 115/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2493 - val_loss: 0.3752\n",
      "Epoch 116/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2476 - val_loss: 0.3744\n",
      "Epoch 117/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2459 - val_loss: 0.3736\n",
      "Epoch 118/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2442 - val_loss: 0.3729\n",
      "Epoch 119/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2425 - val_loss: 0.3723\n",
      "Epoch 120/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2409 - val_loss: 0.3716\n",
      "Epoch 121/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2393 - val_loss: 0.3710\n",
      "Epoch 122/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2377 - val_loss: 0.3702\n",
      "Epoch 123/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2361 - val_loss: 0.3695\n",
      "Epoch 124/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2345 - val_loss: 0.3688\n",
      "Epoch 125/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2330 - val_loss: 0.3681\n",
      "Epoch 126/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2314 - val_loss: 0.3675\n",
      "Epoch 127/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2299 - val_loss: 0.3668\n",
      "Epoch 128/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2284 - val_loss: 0.3660\n",
      "Epoch 129/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2269 - val_loss: 0.3653\n",
      "Epoch 130/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2253 - val_loss: 0.3647\n",
      "Epoch 131/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2239 - val_loss: 0.3640\n",
      "Epoch 132/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2224 - val_loss: 0.3633\n",
      "Epoch 133/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2209 - val_loss: 0.3626\n",
      "Epoch 134/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2195 - val_loss: 0.3619\n",
      "Epoch 135/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2181 - val_loss: 0.3612\n",
      "Epoch 136/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2166 - val_loss: 0.3606\n",
      "Epoch 137/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2152 - val_loss: 0.3599\n",
      "Epoch 138/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2138 - val_loss: 0.3592\n",
      "Epoch 139/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2125 - val_loss: 0.3585\n",
      "Epoch 140/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2111 - val_loss: 0.3578\n",
      "Epoch 141/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2098 - val_loss: 0.3571\n",
      "Epoch 142/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2085 - val_loss: 0.3565\n",
      "Epoch 143/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2071 - val_loss: 0.3558\n",
      "Epoch 144/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2058 - val_loss: 0.3552\n",
      "Epoch 145/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2045 - val_loss: 0.3545\n",
      "Epoch 146/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2032 - val_loss: 0.3538\n",
      "Epoch 147/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.2019 - val_loss: 0.3532\n",
      "Epoch 148/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.2007 - val_loss: 0.3525\n",
      "Epoch 149/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1994 - val_loss: 0.3519\n",
      "Epoch 150/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1982 - val_loss: 0.3513\n",
      "Epoch 151/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1970 - val_loss: 0.3506\n",
      "Epoch 152/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1958 - val_loss: 0.3500\n",
      "Epoch 153/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1947 - val_loss: 0.3494\n",
      "Epoch 154/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1935 - val_loss: 0.3488\n",
      "Epoch 155/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1923 - val_loss: 0.3482\n",
      "Epoch 156/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1912 - val_loss: 0.3477\n",
      "Epoch 157/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1901 - val_loss: 0.3471\n",
      "Epoch 158/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1889 - val_loss: 0.3464\n",
      "Epoch 159/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1878 - val_loss: 0.3458\n",
      "Epoch 160/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1868 - val_loss: 0.3452\n",
      "Epoch 161/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1857 - val_loss: 0.3446\n",
      "Epoch 162/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1846 - val_loss: 0.3440\n",
      "Epoch 163/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1836 - val_loss: 0.3434\n",
      "Epoch 164/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1825 - val_loss: 0.3429\n",
      "Epoch 165/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1815 - val_loss: 0.3423\n",
      "Epoch 166/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1805 - val_loss: 0.3417\n",
      "Epoch 167/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1795 - val_loss: 0.3412\n",
      "Epoch 168/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1785 - val_loss: 0.3406\n",
      "Epoch 169/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1776 - val_loss: 0.3401\n",
      "Epoch 170/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1766 - val_loss: 0.3395\n",
      "Epoch 171/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1756 - val_loss: 0.3390\n",
      "Epoch 172/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1747 - val_loss: 0.3384\n",
      "Epoch 173/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1738 - val_loss: 0.3379\n",
      "Epoch 174/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1729 - val_loss: 0.3373\n",
      "Epoch 175/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1720 - val_loss: 0.3368\n",
      "Epoch 176/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1711 - val_loss: 0.3362\n",
      "Epoch 177/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1702 - val_loss: 0.3357\n",
      "Epoch 178/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1693 - val_loss: 0.3352\n",
      "Epoch 179/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1685 - val_loss: 0.3347\n",
      "Epoch 180/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1676 - val_loss: 0.3342\n",
      "Epoch 181/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1668 - val_loss: 0.3337\n",
      "Epoch 182/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1660 - val_loss: 0.3332\n",
      "Epoch 183/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1651 - val_loss: 0.3327\n",
      "Epoch 184/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1643 - val_loss: 0.3322\n",
      "Epoch 185/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1635 - val_loss: 0.3318\n",
      "Epoch 186/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1628 - val_loss: 0.3313\n",
      "Epoch 187/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1620 - val_loss: 0.3308\n",
      "Epoch 188/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1612 - val_loss: 0.3304\n",
      "Epoch 189/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1605 - val_loss: 0.3299\n",
      "Epoch 190/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1597 - val_loss: 0.3294\n",
      "Epoch 191/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1590 - val_loss: 0.3289\n",
      "Epoch 192/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1582 - val_loss: 0.3285\n",
      "Epoch 193/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1575 - val_loss: 0.3280\n",
      "Epoch 194/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1568 - val_loss: 0.3276\n",
      "Epoch 195/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1561 - val_loss: 0.3272\n",
      "Epoch 196/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1554 - val_loss: 0.3267\n",
      "Epoch 197/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1548 - val_loss: 0.3263\n",
      "Epoch 198/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1541 - val_loss: 0.3258\n",
      "Epoch 199/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1534 - val_loss: 0.3254\n",
      "Epoch 200/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1527 - val_loss: 0.3250\n",
      "Epoch 201/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1520 - val_loss: 0.3246\n",
      "Epoch 202/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1514 - val_loss: 0.3241\n",
      "Epoch 203/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1507 - val_loss: 0.3237\n",
      "Epoch 204/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1501 - val_loss: 0.3233\n",
      "Epoch 205/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1495 - val_loss: 0.3228\n",
      "Epoch 206/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1488 - val_loss: 0.3224\n",
      "Epoch 207/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1482 - val_loss: 0.3220\n",
      "Epoch 208/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1476 - val_loss: 0.3216\n",
      "Epoch 209/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1470 - val_loss: 0.3211\n",
      "Epoch 210/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1464 - val_loss: 0.3207\n",
      "Epoch 211/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1458 - val_loss: 0.3204\n",
      "Epoch 212/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1452 - val_loss: 0.3200\n",
      "Epoch 213/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1446 - val_loss: 0.3196\n",
      "Epoch 214/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1440 - val_loss: 0.3192\n",
      "Epoch 215/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1435 - val_loss: 0.3188\n",
      "Epoch 216/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1429 - val_loss: 0.3184\n",
      "Epoch 217/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1423 - val_loss: 0.3180\n",
      "Epoch 218/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1417 - val_loss: 0.3176\n",
      "Epoch 219/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1412 - val_loss: 0.3172\n",
      "Epoch 220/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1407 - val_loss: 0.3168\n",
      "Epoch 221/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1401 - val_loss: 0.3164\n",
      "Epoch 222/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1396 - val_loss: 0.3160\n",
      "Epoch 223/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1390 - val_loss: 0.3156\n",
      "Epoch 224/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1385 - val_loss: 0.3152\n",
      "Epoch 225/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1380 - val_loss: 0.3148\n",
      "Epoch 226/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1375 - val_loss: 0.3144\n",
      "Epoch 227/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1369 - val_loss: 0.3140\n",
      "Epoch 228/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1364 - val_loss: 0.3136\n",
      "Epoch 229/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1359 - val_loss: 0.3132\n",
      "Epoch 230/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1354 - val_loss: 0.3127\n",
      "Epoch 231/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1349 - val_loss: 0.3123\n",
      "Epoch 232/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1344 - val_loss: 0.3119\n",
      "Epoch 233/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1339 - val_loss: 0.3114\n",
      "Epoch 234/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1334 - val_loss: 0.3110\n",
      "Epoch 235/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1330 - val_loss: 0.3106\n",
      "Epoch 236/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1325 - val_loss: 0.3102\n",
      "Epoch 237/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1320 - val_loss: 0.3098\n",
      "Epoch 238/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1315 - val_loss: 0.3094\n",
      "Epoch 239/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1311 - val_loss: 0.3090\n",
      "Epoch 240/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1306 - val_loss: 0.3086\n",
      "Epoch 241/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1301 - val_loss: 0.3083\n",
      "Epoch 242/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1297 - val_loss: 0.3079\n",
      "Epoch 243/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1292 - val_loss: 0.3076\n",
      "Epoch 244/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1288 - val_loss: 0.3073\n",
      "Epoch 245/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1283 - val_loss: 0.3069\n",
      "Epoch 246/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1279 - val_loss: 0.3066\n",
      "Epoch 247/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1275 - val_loss: 0.3062\n",
      "Epoch 248/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1270 - val_loss: 0.3059\n",
      "Epoch 249/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1266 - val_loss: 0.3056\n",
      "Epoch 250/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1262 - val_loss: 0.3053\n",
      "Epoch 251/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1257 - val_loss: 0.3049\n",
      "Epoch 252/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1253 - val_loss: 0.3046\n",
      "Epoch 253/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1249 - val_loss: 0.3043\n",
      "Epoch 254/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1245 - val_loss: 0.3039\n",
      "Epoch 255/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1241 - val_loss: 0.3036\n",
      "Epoch 256/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1237 - val_loss: 0.3033\n",
      "Epoch 257/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1233 - val_loss: 0.3030\n",
      "Epoch 258/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1229 - val_loss: 0.3027\n",
      "Epoch 259/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1225 - val_loss: 0.3023\n",
      "Epoch 260/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1221 - val_loss: 0.3020\n",
      "Epoch 261/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1217 - val_loss: 0.3017\n",
      "Epoch 262/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1213 - val_loss: 0.3014\n",
      "Epoch 263/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1209 - val_loss: 0.3011\n",
      "Epoch 264/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1206 - val_loss: 0.3008\n",
      "Epoch 265/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1202 - val_loss: 0.3005\n",
      "Epoch 266/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1198 - val_loss: 0.3001\n",
      "Epoch 267/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1194 - val_loss: 0.2999\n",
      "Epoch 268/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1190 - val_loss: 0.2996\n",
      "Epoch 269/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1187 - val_loss: 0.2992\n",
      "Epoch 270/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1183 - val_loss: 0.2989\n",
      "Epoch 271/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1179 - val_loss: 0.2986\n",
      "Epoch 272/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1176 - val_loss: 0.2983\n",
      "Epoch 273/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1172 - val_loss: 0.2980\n",
      "Epoch 274/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1169 - val_loss: 0.2977\n",
      "Epoch 275/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1165 - val_loss: 0.2974\n",
      "Epoch 276/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1162 - val_loss: 0.2971\n",
      "Epoch 277/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1158 - val_loss: 0.2968\n",
      "Epoch 278/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1155 - val_loss: 0.2965\n",
      "Epoch 279/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1151 - val_loss: 0.2962\n",
      "Epoch 280/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1148 - val_loss: 0.2959\n",
      "Epoch 281/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1144 - val_loss: 0.2956\n",
      "Epoch 282/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1141 - val_loss: 0.2953\n",
      "Epoch 283/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1138 - val_loss: 0.2950\n",
      "Epoch 284/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1134 - val_loss: 0.2947\n",
      "Epoch 285/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1131 - val_loss: 0.2944\n",
      "Epoch 286/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1128 - val_loss: 0.2941\n",
      "Epoch 287/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1124 - val_loss: 0.2938\n",
      "Epoch 288/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1121 - val_loss: 0.2935\n",
      "Epoch 289/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1118 - val_loss: 0.2932\n",
      "Epoch 290/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1115 - val_loss: 0.2929\n",
      "Epoch 291/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1111 - val_loss: 0.2927\n",
      "Epoch 292/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1108 - val_loss: 0.2924\n",
      "Epoch 293/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1105 - val_loss: 0.2921\n",
      "Epoch 294/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1102 - val_loss: 0.2918\n",
      "Epoch 295/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1099 - val_loss: 0.2915\n",
      "Epoch 296/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1096 - val_loss: 0.2912\n",
      "Epoch 297/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1093 - val_loss: 0.2909\n",
      "Epoch 298/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1089 - val_loss: 0.2906\n",
      "Epoch 299/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1086 - val_loss: 0.2904\n",
      "Epoch 300/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1083 - val_loss: 0.2901\n",
      "Epoch 301/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1080 - val_loss: 0.2898\n",
      "Epoch 302/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1077 - val_loss: 0.2896\n",
      "Epoch 303/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1074 - val_loss: 0.2893\n",
      "Epoch 304/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1071 - val_loss: 0.2890\n",
      "Epoch 305/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1068 - val_loss: 0.2887\n",
      "Epoch 306/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1065 - val_loss: 0.2885\n",
      "Epoch 307/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1063 - val_loss: 0.2882\n",
      "Epoch 308/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1060 - val_loss: 0.2879\n",
      "Epoch 309/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1057 - val_loss: 0.2876\n",
      "Epoch 310/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1054 - val_loss: 0.2874\n",
      "Epoch 311/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1051 - val_loss: 0.2871\n",
      "Epoch 312/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1048 - val_loss: 0.2868\n",
      "Epoch 313/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1045 - val_loss: 0.2866\n",
      "Epoch 314/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1042 - val_loss: 0.2863\n",
      "Epoch 315/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1039 - val_loss: 0.2861\n",
      "Epoch 316/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1037 - val_loss: 0.2858\n",
      "Epoch 317/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1034 - val_loss: 0.2856\n",
      "Epoch 318/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1031 - val_loss: 0.2853\n",
      "Epoch 319/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1028 - val_loss: 0.2850\n",
      "Epoch 320/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1026 - val_loss: 0.2848\n",
      "Epoch 321/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1023 - val_loss: 0.2845\n",
      "Epoch 322/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.1020 - val_loss: 0.2843\n",
      "Epoch 323/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.1017 - val_loss: 0.2840\n",
      "Epoch 324/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.1015 - val_loss: 0.2837\n",
      "Epoch 325/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1012 - val_loss: 0.2835\n",
      "Epoch 326/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1009 - val_loss: 0.2832\n",
      "Epoch 327/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1007 - val_loss: 0.2830\n",
      "Epoch 328/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1004 - val_loss: 0.2828\n",
      "Epoch 329/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.1001 - val_loss: 0.2825\n",
      "Epoch 330/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0999 - val_loss: 0.2822\n",
      "Epoch 331/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0996 - val_loss: 0.2819\n",
      "Epoch 332/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0993 - val_loss: 0.2817\n",
      "Epoch 333/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0991 - val_loss: 0.2814\n",
      "Epoch 334/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0988 - val_loss: 0.2812\n",
      "Epoch 335/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0986 - val_loss: 0.2809\n",
      "Epoch 336/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0983 - val_loss: 0.2807\n",
      "Epoch 337/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0980 - val_loss: 0.2804\n",
      "Epoch 338/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0978 - val_loss: 0.2802\n",
      "Epoch 339/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0975 - val_loss: 0.2799\n",
      "Epoch 340/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0973 - val_loss: 0.2797\n",
      "Epoch 341/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0970 - val_loss: 0.2794\n",
      "Epoch 342/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0968 - val_loss: 0.2792\n",
      "Epoch 343/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0965 - val_loss: 0.2790\n",
      "Epoch 344/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0962 - val_loss: 0.2787\n",
      "Epoch 345/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0960 - val_loss: 0.2785\n",
      "Epoch 346/600\n",
      "11073/11073 [==============================] - 0s 8us/step - loss: 0.0957 - val_loss: 0.2782\n",
      "Epoch 347/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0955 - val_loss: 0.2780\n",
      "Epoch 348/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0953 - val_loss: 0.2777\n",
      "Epoch 349/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0950 - val_loss: 0.2775\n",
      "Epoch 350/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0948 - val_loss: 0.2773\n",
      "Epoch 351/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0945 - val_loss: 0.2770\n",
      "Epoch 352/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0943 - val_loss: 0.2768\n",
      "Epoch 353/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0940 - val_loss: 0.2765\n",
      "Epoch 354/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0938 - val_loss: 0.2763\n",
      "Epoch 355/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0935 - val_loss: 0.2760\n",
      "Epoch 356/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0933 - val_loss: 0.2758\n",
      "Epoch 357/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0931 - val_loss: 0.2756\n",
      "Epoch 358/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0928 - val_loss: 0.2754\n",
      "Epoch 359/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0926 - val_loss: 0.2751\n",
      "Epoch 360/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0924 - val_loss: 0.2749\n",
      "Epoch 361/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0921 - val_loss: 0.2747\n",
      "Epoch 362/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0919 - val_loss: 0.2745\n",
      "Epoch 363/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0917 - val_loss: 0.2742\n",
      "Epoch 364/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0914 - val_loss: 0.2740\n",
      "Epoch 365/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0912 - val_loss: 0.2738\n",
      "Epoch 366/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0910 - val_loss: 0.2735\n",
      "Epoch 367/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0908 - val_loss: 0.2733\n",
      "Epoch 368/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0905 - val_loss: 0.2731\n",
      "Epoch 369/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0903 - val_loss: 0.2729\n",
      "Epoch 370/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0901 - val_loss: 0.2726\n",
      "Epoch 371/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0899 - val_loss: 0.2724\n",
      "Epoch 372/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0897 - val_loss: 0.2722\n",
      "Epoch 373/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0894 - val_loss: 0.2720\n",
      "Epoch 374/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0892 - val_loss: 0.2717\n",
      "Epoch 375/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0890 - val_loss: 0.2715\n",
      "Epoch 376/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0888 - val_loss: 0.2713\n",
      "Epoch 377/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0886 - val_loss: 0.2711\n",
      "Epoch 378/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0883 - val_loss: 0.2709\n",
      "Epoch 379/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0881 - val_loss: 0.2707\n",
      "Epoch 380/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0879 - val_loss: 0.2704\n",
      "Epoch 381/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0877 - val_loss: 0.2703\n",
      "Epoch 382/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0875 - val_loss: 0.2700\n",
      "Epoch 383/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0873 - val_loss: 0.2698\n",
      "Epoch 384/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0871 - val_loss: 0.2696\n",
      "Epoch 385/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0869 - val_loss: 0.2694\n",
      "Epoch 386/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0867 - val_loss: 0.2692\n",
      "Epoch 387/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0865 - val_loss: 0.2690\n",
      "Epoch 388/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0863 - val_loss: 0.2688\n",
      "Epoch 389/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0861 - val_loss: 0.2686\n",
      "Epoch 390/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0858 - val_loss: 0.2684\n",
      "Epoch 391/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0857 - val_loss: 0.2682\n",
      "Epoch 392/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0855 - val_loss: 0.2680\n",
      "Epoch 393/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0853 - val_loss: 0.2678\n",
      "Epoch 394/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0850 - val_loss: 0.2676\n",
      "Epoch 395/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0849 - val_loss: 0.2674\n",
      "Epoch 396/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0846 - val_loss: 0.2672\n",
      "Epoch 397/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0845 - val_loss: 0.2670\n",
      "Epoch 398/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0843 - val_loss: 0.2668\n",
      "Epoch 399/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0841 - val_loss: 0.2666\n",
      "Epoch 400/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0839 - val_loss: 0.2664\n",
      "Epoch 401/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0837 - val_loss: 0.2663\n",
      "Epoch 402/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0835 - val_loss: 0.2661\n",
      "Epoch 403/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0833 - val_loss: 0.2659\n",
      "Epoch 404/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0831 - val_loss: 0.2657\n",
      "Epoch 405/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0829 - val_loss: 0.2655\n",
      "Epoch 406/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0827 - val_loss: 0.2653\n",
      "Epoch 407/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0826 - val_loss: 0.2651\n",
      "Epoch 408/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0824 - val_loss: 0.2649\n",
      "Epoch 409/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0822 - val_loss: 0.2647\n",
      "Epoch 410/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0820 - val_loss: 0.2646\n",
      "Epoch 411/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0818 - val_loss: 0.2644\n",
      "Epoch 412/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0816 - val_loss: 0.2642\n",
      "Epoch 413/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0814 - val_loss: 0.2640\n",
      "Epoch 414/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0813 - val_loss: 0.2638\n",
      "Epoch 415/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0811 - val_loss: 0.2636\n",
      "Epoch 416/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0809 - val_loss: 0.2634\n",
      "Epoch 417/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0807 - val_loss: 0.2632\n",
      "Epoch 418/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0806 - val_loss: 0.2630\n",
      "Epoch 419/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0804 - val_loss: 0.2628\n",
      "Epoch 420/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0802 - val_loss: 0.2627\n",
      "Epoch 421/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0800 - val_loss: 0.2625\n",
      "Epoch 422/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0798 - val_loss: 0.2623\n",
      "Epoch 423/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0797 - val_loss: 0.2621\n",
      "Epoch 424/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0795 - val_loss: 0.2619\n",
      "Epoch 425/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0793 - val_loss: 0.2618\n",
      "Epoch 426/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0792 - val_loss: 0.2616\n",
      "Epoch 427/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0790 - val_loss: 0.2614\n",
      "Epoch 428/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0788 - val_loss: 0.2612\n",
      "Epoch 429/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0787 - val_loss: 0.2611\n",
      "Epoch 430/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0785 - val_loss: 0.2609\n",
      "Epoch 431/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0783 - val_loss: 0.2607\n",
      "Epoch 432/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0782 - val_loss: 0.2605\n",
      "Epoch 433/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0780 - val_loss: 0.2604\n",
      "Epoch 434/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0778 - val_loss: 0.2602\n",
      "Epoch 435/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0777 - val_loss: 0.2600\n",
      "Epoch 436/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0775 - val_loss: 0.2599\n",
      "Epoch 437/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0773 - val_loss: 0.2597\n",
      "Epoch 438/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0772 - val_loss: 0.2595\n",
      "Epoch 439/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0770 - val_loss: 0.2593\n",
      "Epoch 440/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0769 - val_loss: 0.2591\n",
      "Epoch 441/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0767 - val_loss: 0.2590\n",
      "Epoch 442/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0765 - val_loss: 0.2589\n",
      "Epoch 443/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0764 - val_loss: 0.2587\n",
      "Epoch 444/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0762 - val_loss: 0.2585\n",
      "Epoch 445/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0761 - val_loss: 0.2583\n",
      "Epoch 446/600\n",
      "11073/11073 [==============================] - 0s 11us/step - loss: 0.0759 - val_loss: 0.2582\n",
      "Epoch 447/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0758 - val_loss: 0.2580\n",
      "Epoch 448/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0756 - val_loss: 0.2579\n",
      "Epoch 449/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0755 - val_loss: 0.2577\n",
      "Epoch 450/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0753 - val_loss: 0.2575\n",
      "Epoch 451/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0751 - val_loss: 0.2574\n",
      "Epoch 452/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0750 - val_loss: 0.2572\n",
      "Epoch 453/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0748 - val_loss: 0.2570\n",
      "Epoch 454/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0747 - val_loss: 0.2569\n",
      "Epoch 455/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0745 - val_loss: 0.2567\n",
      "Epoch 456/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0744 - val_loss: 0.2566\n",
      "Epoch 457/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0742 - val_loss: 0.2564\n",
      "Epoch 458/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0741 - val_loss: 0.2563\n",
      "Epoch 459/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0740 - val_loss: 0.2561\n",
      "Epoch 460/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0738 - val_loss: 0.2560\n",
      "Epoch 461/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0737 - val_loss: 0.2558\n",
      "Epoch 462/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0735 - val_loss: 0.2557\n",
      "Epoch 463/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0734 - val_loss: 0.2555\n",
      "Epoch 464/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0732 - val_loss: 0.2554\n",
      "Epoch 465/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0731 - val_loss: 0.2552\n",
      "Epoch 466/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0729 - val_loss: 0.2551\n",
      "Epoch 467/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0728 - val_loss: 0.2549\n",
      "Epoch 468/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0727 - val_loss: 0.2547\n",
      "Epoch 469/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0725 - val_loss: 0.2546\n",
      "Epoch 470/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0724 - val_loss: 0.2544\n",
      "Epoch 471/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0722 - val_loss: 0.2543\n",
      "Epoch 472/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0721 - val_loss: 0.2541\n",
      "Epoch 473/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0720 - val_loss: 0.2540\n",
      "Epoch 474/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0718 - val_loss: 0.2539\n",
      "Epoch 475/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0717 - val_loss: 0.2537\n",
      "Epoch 476/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0716 - val_loss: 0.2536\n",
      "Epoch 477/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0714 - val_loss: 0.2535\n",
      "Epoch 478/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0713 - val_loss: 0.2533\n",
      "Epoch 479/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0712 - val_loss: 0.2531\n",
      "Epoch 480/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0710 - val_loss: 0.2530\n",
      "Epoch 481/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0709 - val_loss: 0.2529\n",
      "Epoch 482/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0708 - val_loss: 0.2527\n",
      "Epoch 483/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0706 - val_loss: 0.2526\n",
      "Epoch 484/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0705 - val_loss: 0.2525\n",
      "Epoch 485/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0704 - val_loss: 0.2523\n",
      "Epoch 486/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0702 - val_loss: 0.2522\n",
      "Epoch 487/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0701 - val_loss: 0.2521\n",
      "Epoch 488/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0700 - val_loss: 0.2519\n",
      "Epoch 489/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0699 - val_loss: 0.2518\n",
      "Epoch 490/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0697 - val_loss: 0.2517\n",
      "Epoch 491/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0696 - val_loss: 0.2515\n",
      "Epoch 492/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0695 - val_loss: 0.2514\n",
      "Epoch 493/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0693 - val_loss: 0.2513\n",
      "Epoch 494/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0692 - val_loss: 0.2511\n",
      "Epoch 495/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0691 - val_loss: 0.2510\n",
      "Epoch 496/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0690 - val_loss: 0.2509\n",
      "Epoch 497/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0688 - val_loss: 0.2508\n",
      "Epoch 498/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0687 - val_loss: 0.2506\n",
      "Epoch 499/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0686 - val_loss: 0.2505\n",
      "Epoch 500/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0685 - val_loss: 0.2503\n",
      "Epoch 501/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0684 - val_loss: 0.2502\n",
      "Epoch 502/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0682 - val_loss: 0.2501\n",
      "Epoch 503/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0681 - val_loss: 0.2500\n",
      "Epoch 504/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0680 - val_loss: 0.2498\n",
      "Epoch 505/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0679 - val_loss: 0.2497\n",
      "Epoch 506/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0678 - val_loss: 0.2496\n",
      "Epoch 507/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0676 - val_loss: 0.2495\n",
      "Epoch 508/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0675 - val_loss: 0.2493\n",
      "Epoch 509/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0674 - val_loss: 0.2492\n",
      "Epoch 510/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0672 - val_loss: 0.2491\n",
      "Epoch 511/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0672 - val_loss: 0.2490\n",
      "Epoch 512/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0670 - val_loss: 0.2489\n",
      "Epoch 513/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0669 - val_loss: 0.2488\n",
      "Epoch 514/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0668 - val_loss: 0.2486\n",
      "Epoch 515/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0667 - val_loss: 0.2485\n",
      "Epoch 516/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0666 - val_loss: 0.2484\n",
      "Epoch 517/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0664 - val_loss: 0.2483\n",
      "Epoch 518/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0663 - val_loss: 0.2482\n",
      "Epoch 519/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0662 - val_loss: 0.2481\n",
      "Epoch 520/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0661 - val_loss: 0.2480\n",
      "Epoch 521/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0660 - val_loss: 0.2478\n",
      "Epoch 522/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0659 - val_loss: 0.2477\n",
      "Epoch 523/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0658 - val_loss: 0.2476\n",
      "Epoch 524/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0657 - val_loss: 0.2475\n",
      "Epoch 525/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0655 - val_loss: 0.2474\n",
      "Epoch 526/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0654 - val_loss: 0.2473\n",
      "Epoch 527/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0653 - val_loss: 0.2472\n",
      "Epoch 528/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0652 - val_loss: 0.2471\n",
      "Epoch 529/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0651 - val_loss: 0.2469\n",
      "Epoch 530/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0650 - val_loss: 0.2468\n",
      "Epoch 531/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0649 - val_loss: 0.2467\n",
      "Epoch 532/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0648 - val_loss: 0.2466\n",
      "Epoch 533/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0647 - val_loss: 0.2465\n",
      "Epoch 534/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0646 - val_loss: 0.2464\n",
      "Epoch 535/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0645 - val_loss: 0.2463\n",
      "Epoch 536/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0644 - val_loss: 0.2462\n",
      "Epoch 537/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0642 - val_loss: 0.2461\n",
      "Epoch 538/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0641 - val_loss: 0.2460\n",
      "Epoch 539/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0640 - val_loss: 0.2459\n",
      "Epoch 540/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0639 - val_loss: 0.2457\n",
      "Epoch 541/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0638 - val_loss: 0.2456\n",
      "Epoch 542/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0637 - val_loss: 0.2455\n",
      "Epoch 543/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0636 - val_loss: 0.2454\n",
      "Epoch 544/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0635 - val_loss: 0.2453\n",
      "Epoch 545/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0634 - val_loss: 0.2452\n",
      "Epoch 546/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0633 - val_loss: 0.2451\n",
      "Epoch 547/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0632 - val_loss: 0.2450\n",
      "Epoch 548/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0631 - val_loss: 0.2449\n",
      "Epoch 549/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0630 - val_loss: 0.2448\n",
      "Epoch 550/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0629 - val_loss: 0.2447\n",
      "Epoch 551/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0628 - val_loss: 0.2446\n",
      "Epoch 552/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0627 - val_loss: 0.2445\n",
      "Epoch 553/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0626 - val_loss: 0.2444\n",
      "Epoch 554/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0625 - val_loss: 0.2443\n",
      "Epoch 555/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0624 - val_loss: 0.2442\n",
      "Epoch 556/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0623 - val_loss: 0.2441\n",
      "Epoch 557/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0622 - val_loss: 0.2440\n",
      "Epoch 558/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0621 - val_loss: 0.2439\n",
      "Epoch 559/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0620 - val_loss: 0.2438\n",
      "Epoch 560/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0619 - val_loss: 0.2437\n",
      "Epoch 561/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0618 - val_loss: 0.2436\n",
      "Epoch 562/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0617 - val_loss: 0.2435\n",
      "Epoch 563/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0616 - val_loss: 0.2434\n",
      "Epoch 564/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0615 - val_loss: 0.2433\n",
      "Epoch 565/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0614 - val_loss: 0.2432\n",
      "Epoch 566/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0613 - val_loss: 0.2431\n",
      "Epoch 567/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0612 - val_loss: 0.2430\n",
      "Epoch 568/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0611 - val_loss: 0.2429\n",
      "Epoch 569/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0610 - val_loss: 0.2428\n",
      "Epoch 570/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0609 - val_loss: 0.2427\n",
      "Epoch 571/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0609 - val_loss: 0.2427\n",
      "Epoch 572/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0607 - val_loss: 0.2426\n",
      "Epoch 573/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0607 - val_loss: 0.2425\n",
      "Epoch 574/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0606 - val_loss: 0.2424\n",
      "Epoch 575/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0605 - val_loss: 0.2423\n",
      "Epoch 576/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0604 - val_loss: 0.2422\n",
      "Epoch 577/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0603 - val_loss: 0.2421\n",
      "Epoch 578/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0602 - val_loss: 0.2420\n",
      "Epoch 579/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0601 - val_loss: 0.2420\n",
      "Epoch 580/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0600 - val_loss: 0.2419\n",
      "Epoch 581/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0599 - val_loss: 0.2418\n",
      "Epoch 582/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0598 - val_loss: 0.2417\n",
      "Epoch 583/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0598 - val_loss: 0.2416\n",
      "Epoch 584/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0597 - val_loss: 0.2415\n",
      "Epoch 585/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0596 - val_loss: 0.2414\n",
      "Epoch 586/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0595 - val_loss: 0.2413\n",
      "Epoch 587/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0594 - val_loss: 0.2412\n",
      "Epoch 588/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0593 - val_loss: 0.2412\n",
      "Epoch 589/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0592 - val_loss: 0.2411\n",
      "Epoch 590/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0591 - val_loss: 0.2410\n",
      "Epoch 591/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0590 - val_loss: 0.2409\n",
      "Epoch 592/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0590 - val_loss: 0.2408\n",
      "Epoch 593/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0589 - val_loss: 0.2407\n",
      "Epoch 594/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0588 - val_loss: 0.2407\n",
      "Epoch 595/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0587 - val_loss: 0.2406\n",
      "Epoch 596/600\n",
      "11073/11073 [==============================] - 0s 10us/step - loss: 0.0586 - val_loss: 0.2405\n",
      "Epoch 597/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0585 - val_loss: 0.2404\n",
      "Epoch 598/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0584 - val_loss: 0.2403\n",
      "Epoch 599/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0584 - val_loss: 0.2402\n",
      "Epoch 600/600\n",
      "11073/11073 [==============================] - 0s 9us/step - loss: 0.0583 - val_loss: 0.2402\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "reduce =  ReduceLROnPlateau(patience=5, factor=0.2, verbose=1)\n",
    "\n",
    "history = model.fit(train_x, train_x, \n",
    "                    batch_size=512, epochs=600, \n",
    "                    validation_data=(test_x,test_x), \n",
    "                    callbacks=[reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotit(history, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "infected = pd.read_csv('input/data_infected_direct.csv', parse_dates=['start_time']).sort_values('start_time').drop('start_time', axis=1)\n",
    "infected.fillna(0, inplace=True)\n",
    "infected_x = scaler.transform(infected[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = pd.read_csv('input/data_after_direct.csv', parse_dates=['start_time']).sort_values('start_time').drop('start_time', axis=1)\n",
    "after.fillna(0, inplace=True)\n",
    "after_x = scaler.transform(after[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_pred = model.predict(infected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pred = model.predict(after_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05079323293079824\n",
      "0.2242760406172531\n",
      "0.3398167032728574\n",
      "0.12555553645103648\n"
     ]
    }
   ],
   "source": [
    "train_error = ((train_x - train_pred) ** 2).mean(axis=-1) \n",
    "print(train_error.mean())\n",
    "test_error = ((test_x - test_pred) ** 2).mean(axis=-1) \n",
    "print(test_error.mean())\n",
    "infected_error = ((infected_x - i_pred) ** 2).mean(axis=-1)\n",
    "print(infected_error.mean())\n",
    "after_error = ((after_x - a_pred) ** 2).mean(axis=-1)\n",
    "print(after_error.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52373,)\n"
     ]
    }
   ],
   "source": [
    "errors = np.concatenate([train_error, test_error, infected_error, after_error])\n",
    "print(errors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_line = len(train_x)\n",
    "infected_line = len(train_x) + len(test_x)\n",
    "after_line = len(train_x) + len(test_x) + len(infected_x)\n",
    "plt.plot(running_mean((errors>0.42).astype(int), 50)) \n",
    "# plt.plot(running_mean(errors, 200)) \n",
    "plt.title('Errores sobre el limite')\n",
    "plt.axvline(x=train_line, color='green')\n",
    "plt.axvline(x=infected_line, color='red')\n",
    "plt.axvline(x=after_line, color='red')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('Tiempo')\n",
    "# plt.legend(['Threshold','Error', ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0018061952497064934,\n",
       " 0.023284313725490197,\n",
       " 0.17444826061822016,\n",
       " 0.008807509560783406)"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((train_error > 0.42).astype(int).mean(),\n",
    "(test_error > 0.42).astype(int).mean(),\n",
    "(infected_error > 0.42).astype(int).mean(),\n",
    "(after_error > 0.42).astype(int).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03740600474831735,\n",
       " 0.21569173644547074,\n",
       " 0.1394629741187928,\n",
       " 0.09294705504885593)"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.median(train_error),\n",
    "np.median(test_error),\n",
    "np.median(infected_error),\n",
    "np.median(after_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03740600474831735,\n",
       " 0.21569173644547074,\n",
       " 0.1394629741187928,\n",
       " 0.09294705504885593)"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.median(train_error),\n",
    "np.median(test_error),\n",
    "np.median(infected_error),\n",
    "np.median(after_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "all_arr = [test_error, infected_error, after_error]\n",
    "# ax = sns.boxplot(data=all_arr)\n",
    "ax = sns.boxplot(data=after_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
